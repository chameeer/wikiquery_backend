# 分布式运行环境

深脑云服务器（学院官方服务器）

下面的SSH，Hadoop伪分布式可以不安装

Scala未安装

## 下载相关的包和数据
```bash
cd ~/Desktop/分布式框架安装
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz # Hadoop
wget https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz #Spark
wget https://downloads.lightbend.com/scala/2.12.8/scala-2.12.8.deb # Scala
cd ~/Desktop/Wiki_Query
https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2 # 数据
```
并上传`jdk-8u162-linux-x64.tar.gz`

## 配置SSH

```bash
apt-get install openssh-server
ssh localhost #回车
exit
cd ~/.ssh/ 
ssh-keygen -t rsa #回车
cat ./id_rsa.pub >> ./authorized_keys
```

## 配置Java
```bash
cd /usr/lib
mkdir jvm 
cd ~/Desktop/分布式框架安装
tar -zxvf ./jdk-8u162-linux-x64.tar.gz -C /usr/lib/jvm 
vim ~/.bashrc
#补充如下内容
export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
#
source ~/.bashrc
java -version #查看版本信息
```

## 配置Hadoop伪分布式
```bash
cd ~/Desktop/分布式框架安装
tar -zxf hadoop-3.1.3.tar.gz -C /usr/local
cd /usr/local/
mv ./hadoop-3.1.3/ ./hadoop
cd /usr/local/hadoop
./bin/hadoop version #查看版本信息
```

```bash
cd /usr/local/hadoop
vim ./etc/hadoop/core-site.xml
# 修改
<configuration>
</configuration>
# 为
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
#

vim ./etc/hadoop/hdfs-site.xml
# 修改
<configuration>
</configuration>
# 为
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/data</value>
    </property>
</configuration>
#
```

```bash
vim ~/.bashrc
# 开头补充如下内容
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:
#
source ~/.bashrc
```

```bash
cd /usr/local/hadoop
vim sbin/start-dfs.sh
# 开头补充如下内容
HDFS_DATANODE_USER=root
HADOOP_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
#

vim sbin/stop-dfs.sh
# 开头补充如下内容
HDFS_DATANODE_USER=root
HADOOP_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
#

vim sbin/start-yarn.sh
# 开头补充如下内容
YARN_RESOURCEMANAGER_USER=root
HADOOP_DATANODE_SECURE_USER=yarn
YARN_NODEMANAGER_USER=root
#

vim sbin/stop-yarn.sh
# 开头补充如下内容
YARN_RESOURCEMANAGER_USER=root
HADOOP_DATANODE_SECURE_USER=yarn
YARN_NODEMANAGER_USER=root
#
```

```bash
./bin/hdfs namenode -format
sbin/start-all.sh
jps
sbin/stop-all.sh
# 验证无误即可
```

## 配置Spark

```bash
cd ~/Desktop/分布式框架安装
sudo tar -zxf spark-2.4.8-bin-hadoop2.7.tgz -C /usr/local/
cd /usr/local/
ln -s spark-2.4.3-bin-hadoop2.7 spark
```

```bash
cd spark/conf
cp spark-env.sh.template spark-env.sh
vim spark-env.sh
#
export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export SPARK_MASTER_IP=hadoop1
export SPARK_MASTER_PORT=7077
#
```

```bash
vim ~/.bashrc
# 补充如下内容
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin
#
source ~/.bashrc
```